{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tBfp8I4bO-CF"},"source":["# **Tokenizer**"]},{"cell_type":"code","metadata":{"id":"DA0BUTSjO7iD","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"50733aa3-f53e-47ed-c262-776d11fc60ed"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","    'i love my dog',\n","    'I, love my cat',\n","    'You love my dog!'\n","    'Do you think my dog is amazing?'\n","]\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wGC-lr8tQNzv"},"source":["# **Padding**"]},{"cell_type":"code","metadata":{"id":"COrgYp6dQPoH"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","sentences = [\n","    'I love my dog',\n","    'I love my cat',\n","    'You love my dog!',\n","    'Do you think my dog is amazing?'\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJAn6hSgRU49","colab":{"base_uri":"https://localhost:8080/","height":167},"outputId":"ea751d54-edb3-40e5-e87a-6a2e18d33152"},"source":["tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","padded = pad_sequences(sequences, maxlen=5)\n","print(\"\\nWord Index = \" , word_index)\n","print(\"\\nSequences = \" , sequences)\n","print(\"\\nPadded Sequences:\")\n","print(padded)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","\n","Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4, 8, 6, 9, 2, 4, 10, 11]]\n","\n","Padded Sequences:\n","[[ 0  5  3  2  4]\n"," [ 0  5  3  2  7]\n"," [ 9  2  4 10 11]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G8yqH7PZR1YQ","colab":{"base_uri":"https://localhost:8080/","height":117},"outputId":"9031c574-3052-4559-891b-033960cb2a2c"},"source":["# Try with words that the tokenizer wasn't fit to\n","test_data = [\n","    'i really love my dog',\n","    'my dog loves my house much'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(\"\\nTest Sequence = \", test_seq)\n","\n","padded = pad_sequences(test_seq, maxlen=10,padding='pre')\n","print(\"\\nPadded Test Sequence: \")\n","print(padded)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1, 1]]\n","\n","Padded Test Sequence: \n","[[0 0 0 0 0 5 1 3 2 4]\n"," [0 0 0 0 2 4 1 2 1 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","source":["#Stopword Removal"],"metadata":{"id":"LOHa1g9KSFjd"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Download resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","# Download punkt_tab specifically\n","nltk.download('punkt_tab')\n","\n","\n","text = \"This is a simple example to demonstrate stopword removal.\"\n","\n","# Tokenize\n","words = word_tokenize(text)\n","\n","# Remove stopwords\n","stop_words = set(stopwords.words('english'))\n","filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","print(\"Original:\", words)\n","print(\"Without Stopwords:\", filtered_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_vBZw-kpSFOF","outputId":"2e381c99-6b12-47a9-9b94-1f5fe933b036"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original: ['This', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'stopword', 'removal', '.']\n","Without Stopwords: ['simple', 'example', 'demonstrate', 'stopword', 'removal', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["#Stemming\n","\n","Used to reduce words to their root or base form, often called a stem."],"metadata":{"id":"f6kEr8ZvSXBp"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","ps = PorterStemmer()\n","words = [\"running\", \"runs\", \"easily\", \"fairly\"]\n","\n","stemmed_words = [ps.stem(word) for word in words]\n","print(\"Stemmed Words:\", stemmed_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8olqiQySZeP","outputId":"f4b77f69-42ae-4c98-c82f-53c9a1f366f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stemmed Words: ['run', 'run', 'easili', 'fairli']\n"]}]},{"cell_type":"markdown","source":["#Lemmatization\n","\n","It's used to tranform the words to their base dictionary form"],"metadata":{"id":"-3tYBOW0Sqyb"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","\n","# Download WordNet\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","lemmatizer = WordNetLemmatizer()\n","words = [\"running\", \"better\", \"easily\", \"fairly\"]\n","\n","lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # 'v' = verb\n","print(\"Lemmatized Words:\", lemmatized_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__wLU1DeSuOa","outputId":"a9fd010f-dc80-46a7-9edf-2cf544cdf6e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Lemmatized Words: ['run', 'better', 'easily', 'fairly']\n"]}]},{"cell_type":"markdown","source":["#n-gram precision"],"metadata":{"id":"xi9F7w3yTVQK"}},{"cell_type":"code","source":["from nltk.util import ngrams\n","\n","text = \"I love natural language processing\"\n","tokens = text.split()\n","\n","# Generate bigrams (n=2)\n","bigrams = list(ngrams(tokens, 2))\n","print(\"Bigrams:\", bigrams)\n","\n","# Generate trigrams (n=3)\n","trigrams = list(ngrams(tokens, 3))\n","print(\"Trigrams:\", trigrams)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yY4nQQYpTUnb","outputId":"bc623bcf-57f9-4723-ac43-dc29760f1867"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bigrams: [('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing')]\n","Trigrams: [('I', 'love', 'natural'), ('love', 'natural', 'language'), ('natural', 'language', 'processing')]\n"]}]}]}